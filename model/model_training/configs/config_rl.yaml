defaults_rlhf:
  datasets:
  batch_size: 18
  epochs: 20
  datasets_extra: []
  cache_dir: .cache
  output_dir: model_rl
  eval_size: 500
  rank_config:
  sft_config:

oasst_export_latin_cyrillic_rlhf:
  datasets:
    - oasst_export:
        lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
        #top_k: 2
        input_file_path: 2023-03-07_oasst_default_with_labels.jsonl.gz
  sort_by_length: false
  use_custom_sampler: false

pythia_rlhf:
  # model_name: EleutherAI/pythia-1b-deduped-base-finetuned/checkpoint-2000
  rank_config:
    is_reward_model: true
    model_name: pythia_rm/checkpoint-1000/
    cache_dir:
    pooling: last
    residual_dropout: 0.0
    use_flash_attention: false

  sft_config:
    is_reward_model: false
    model_name: pythia_sft/checkpoint-2000/
    cache_dir:
    quantization: false
    seq2seqmodel: False
    freeze_layer:
    residual_dropout: 0.0
    use_flash_attention: false

  batch_size: 18

debug_rlhf:
  # model_name: gpt2
  rank_model: pythia_reward_model/checkpoint-50
  sft_model: pythia_sft/checkpoint-10/
  batch_size: 2
  log_dir: test

defaults_rlhf:
  dataset:
  rank_model: TODO
  sft_model: TODO
  eval_prompts:
  batch_size: 18
  epochs: 10
  datasets_extra: []
  cache_dir: .cache
  quantization: false
  seq2seqmodel: false
  eval_size:
  output_dir: model_rl

oasst_export_latin_cyrillic_rlhf:
  datasets:
    - oasst_export:
        lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
        #top_k: 2
        input_file_path: 2023-03-07_oasst_default_with_labels.jsonl.gz
  sort_by_length: false
  use_custom_sampler: false

pythia_rlhf:
  # model_name: EleutherAI/pythia-1b-deduped-base-finetuned/checkpoint-2000
  rank_model: pythia_rm/checkpoint-1000/
  rank_model_seq2seq: false
  sft_model: pythia_sft/checkpoint-2000/
  sft_model_seq2seq: false
  batch_size: 18

debug_rlhf:
  # model_name: gpt2
  rank_model: pythia_reward_model/checkpoint-50
  sft_model: pythia_sft/checkpoint-10/
  batch_size: 2
  log_dir: test
